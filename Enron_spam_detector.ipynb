{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Spam Detector:\n",
    "The steps that lead to the classification are:\n",
    "- Extraction of messages from emails in the training set,\n",
    "- Converting the messages into a vectors,\n",
    "- Train a model based on these vectors,\n",
    "- Extract a message from test email, convert the message into vector\n",
    "- With the help of the trained model, predict the email to be either 'ham' or 'spam'\n",
    "\n",
    "Let us go through every step and analyze how can we get more than 98% accuracy by training a set of 6600 emails. The data that we are using Enron email data which can be downloaded from http://csmining.org/index.php/enron-spam-datasets.html. I have downloaded the first two folders of preprocessed data and divided the files into 60% train and 40% test sets. You can download the Raw data and use that as the input. The results wont change, but the running time is the cost we must pay.<br>\n",
    "<br>\n",
    "So the whole process boils down to three important steps:\n",
    "- **Extraction of messages;**\n",
    "- **Convertion to vectors;**\n",
    "- **Choosing a model for classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting messages:\n",
    "Every text file once opened, extraction of the message and subject of email can be done using **email** library. These messages can be converted into a vector and directly fed into the training model. But think of those spam emails which contain hypertexts and weblinks. All these links are converted into sparse vectors. Sparse because weblinks that appear in one email mostly wont appear in others. But the presence of a weblink and the number of them present in an email can be a good parameter to classify it as a spam or not. So we can extract the body of email without weblinks and for every weblink present in the body, we can append the word *'link'* at the end of body. As we can see later, the frequency of a word appearing in the email is also considered while classifying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_file_path :\n",
    "- give *dir_path* as input,\n",
    "- use os.walk, and for each file in the directory append the file_path into the array *files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def extract_file_path(files, dir_path):\n",
    "    for root, dir_name, file_name in os.walk(dir_path): # gives the root, directory name and file name for each file in the directory mentioned.\n",
    "        for name in file_name:\n",
    "            file_path = [os.path.join(root, name)] # joins the root name and file name to give the file path\n",
    "            files = np.hstack((files, file_path)) # append the file path to the array files.\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### msg_sub_and_tag_of_email :\n",
    "- use *codecs.open to open the *email_file*, with 'ISO-8859-1' encoding and read the opened file }= *raw_email*\n",
    "- use *email.message_from_string* to get the message and subject }= *mail*\n",
    "- *mail.get_payload()* gives message }= *msg*\n",
    "- *mail['subject']* gives subject of the email.}= *sub*\n",
    "- extract the weblinks using *lxml.html* and use *BeautifulSoup* to scoop out weblinks and give the remaining message.\n",
    "- as the data is already segregated into two folders with names 'ham' and 'spam', we check for each file the folder it is in and tag 0 = spam; 1 = ham.\n",
    "\n",
    "-- encoding = 'ISO-8859-1', as 'utf-8' can not read symbol in some of the spam messages.<br>\n",
    "-- We can use *BeautifulSoup* to extract weblinks, but for that the file format must be .html or .xml. *BeautifulSoup* gives error if it is a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, email, lxml.html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def msg_sub_and_tag_of_email(email_file):\n",
    "    \n",
    "    data = codecs.open(email_file, encoding='ISO-8859-1', errors='ignore')\n",
    "    raw_email = data.read()# open the file and read the text\n",
    "    mail = email.message_from_string(raw_email) # extract the contents of the file.\n",
    "    \n",
    "    sub = mail['subject'] # from the contents, extract the subject.\n",
    "    if sub == None:\n",
    "        sub = ' '         # if there is no subject, take the subject as an empy string\n",
    "    \n",
    "    msg = mail.get_payload()  # get_payload is a module in the library 'email' which gives bosy of the email.\n",
    "    #while isinstance(msg, list): \n",
    "     #   msg = msg[0].get_payload()\n",
    "    \n",
    "    dom =  lxml.html.fromstring(raw_email) # from the string 'raw_email' 'dom' is created to calculate the number of weblinks\n",
    "    msg = msg + (' link'*len(dom.xpath('//a/@href'))) # for every weblink, append the word ' link' at the end of message.\n",
    "    soup = BeautifulSoup(msg,'html.parser' )\n",
    "    msg = soup.get_text() # extract the rest of message after deleting the weblinks \n",
    "    \n",
    "    number_of_folders_to_be_opened = 7\n",
    "    tag = 0\n",
    "    if ((email_file.split('/')[number_of_folders_to_be_opened]).lower() == 'ham'): # look for the filder in which the 'email_file' is stored \n",
    "        tag = 1 # if the 'email_file' is in ham folder, tag = 1, if not, the file is in spam folder, so tag = 0.\n",
    "    \n",
    "    data.close()\n",
    "    return [tag, sub + ' \\n ' + msg ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "file_list = extract_file_path(file_list, '__dir_path__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = np.random.permutation(len(file_list)) # before splitting into train and test sets, we must jumble the 'file_list'. \n",
    "                                                     # This takes away the possibility of train set having majority ham or spam messages.\n",
    "train_size = int(0.6*len(file_list)) # 60% train and 40% test sets.\n",
    "file_list_train = file_list[permutations[:train_size]]\n",
    "file_list_test = file_list[permutations[train_size:len(file_list)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_msg_and_sub = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(file_list_train)):\n",
    "    m_s_t_of_email = msg_sub_and_tag_of_email(file_list_train[i])\n",
    "    train_msg_and_sub = np.hstack((train_msg_and_sub, m_s_t_of_email[1]))\n",
    "    y_train = np.hstack((y_train, m_s_t_of_email[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_msg_and_sub = []\n",
    "y_test = []\n",
    "\n",
    "for j in range(len(file_list_test)):\n",
    "    m_s_t_of_email = msg_sub_and_tag_of_email(file_list_test[j])\n",
    "    test_msg_and_sub = np.hstack((test_msg_and_sub, m_s_t_of_email[1]))\n",
    "    y_test = np.hstack((y_test, m_s_t_of_email[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting messages into vector:\n",
    "Once we have the messages and subject extracted from emails, the next step would be converting these messages into vecots. One way to do that would be:\n",
    "- splitting each message and subject into words,\n",
    "- appending all the words into an array.\n",
    "- and dropping repeated words.\n",
    "\n",
    "For example 'This is a test message' after splitting becomes an array ['This', 'is', 'a', 'test', 'message']. The next email will also be split into words and appended to this array. This procees gives the total vocabulary, which has repetition of words. *np.unique* takes care of this issue. \n",
    "Then for each email, we consider a vector of length V (size of vocabulary) all zeros, and 1 for every word present in the email and vocabulary.<br>\n",
    "The downside of this process of vectorizing the messages is it takes a ot of time. Instead we can use **CountVectorizer** which converts the whole training set into a sparse matrix. While using *CountVectorizer*, define *min_df* feature to be equal to 1/train_size. This improves the accuracy because if there is a word or number that is used only in one email to denote the cost of something, or time of rendezvous, or name of the sender, or any such thing, it adds less information about whether the email is spam or ham. Adding this feature improved the classification from 98.4% to 98.6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df=1/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = cv.fit_transform(np.array(train_msg_and_sub))\n",
    "x_test = cv.transform(np.array(test_msg_and_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model:\n",
    " Initially I have used the former meathod of appending each word to the vocabulary array and using that vocab to convert messages into vectors. When i did that I have used Gaussian Naive Bayes (**GaussianNB**) classifier. GaussianNB does not care about the repetition of a word in a message. It is only bothered about the presence of it. When I switched to *CountVectorizer*, however, does consider repetitions. So instead of using a *GuassianNB* I used **MultinomialNB** classifier. This improved the classification accuracy from 96.5% for *GaussianNB* to 98.6% for *MultinomialNB*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model_multinb = MultinomialNB()\n",
    "model_multinb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98300090661831374"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_multinb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1178,   48],\n",
       "       [  27, 3159]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = confusion_matrix(y_test, y_pred)[1,1]\n",
    "true_negative = confusion_matrix(y_test, y_pred)[0,0]\n",
    "predicted_positive = (y_pred == 1).sum()\n",
    "positive_class = (y_test == 1).sum()\n",
    "accuracy = (true_positive + true_negative)/len(y_test)\n",
    "recall = true_positive/positive_class\n",
    "precision = true_positive/predicted_positive\n",
    "f1_score = 2*recall*precision/(recall + precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98300090661831374"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99152542372881358"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98503274087932646"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98826841858282499"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
