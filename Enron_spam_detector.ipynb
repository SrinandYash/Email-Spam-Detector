{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Spam Detector:\n",
    "The steps that lead to the classification are:\n",
    "- Extraction of messages from emails in the training set,\n",
    "- Converting the messages into vectors,\n",
    "- Train a model based on these vectors,\n",
    "- Extract a message from test email, convert the message into vector\n",
    "- With the help of the trained model, predict the email to be either 'ham' or 'spam.'\n",
    "\n",
    "Let us go through every step and analyze how can we get more than 98% accuracy by training a set of 6600 emails. The data that we are using Enron email data which can be downloaded from http://csmining.org/index.php/enron-spam-datasets.html. I have downloaded the first two folders of preprocessed data and divided the files into 60% train and 40% test sets. You can download the Raw data and use that as the input. The results won't change, but the running time is the cost we must pay.<br>\n",
    "<br>\n",
    "So the whole process of classification boils down to three important steps:\n",
    "- **Preprocessing of Data**\n",
    "- **Transformation of Data**\n",
    "- **Training a Model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODULES\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs, email, lxml.html\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Data:\n",
    "- Every text file once opened, extraction of the message and subject of email can be done using **email** library. \n",
    "- If body of the email contains any web links, remove the links and add a word 'link' at the end of the message.\n",
    "- Do this for every email, and append message and subject of each email to form the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_path(files, dir_path):\n",
    "    '''Extracts the path of each email text file and appends all these into and array 'files'.\n",
    "       Later, the classifier can be run on this array any number of times'''\n",
    "    for root, dir_name, file_name in os.walk(dir_path): \n",
    "        for name in file_name:\n",
    "            file_path = [os.path.join(root, name)]\n",
    "            files = np.hstack((files, file_path)) \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/srinandyash/Downloads/data/enron/'#give the path to the folder where you have stored the email files.\n",
    "file_list = []\n",
    "file_list = extract_file_path(file_list, dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_email(email_file):\n",
    "    data = codecs.open(email_file, encoding='ISO-8859-1', errors='ignore')\n",
    "    raw_email = data.read()\n",
    "    mail = email.message_from_string(raw_email)\n",
    "    data.close()\n",
    "    return mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoop_out_weblinks(msg_with_links):\n",
    "    dom =  lxml.html.fromstring(msg_with_links) \n",
    "    msg_with_links = msg_with_links + (' link'*len(dom.xpath('//a/@href'))) \n",
    "    soup = BeautifulSoup(msg_with_links,'html.parser' )\n",
    "    msg_without_links = soup.get_text() \n",
    "    return msg_without_links\n",
    "\n",
    "def get_msg(mail):\n",
    "    msg = mail.get_payload()\n",
    "    while isinstance(msg,list):\n",
    "        msg = msg[0].get_payload()\n",
    "    if (len(msg) != 0):\n",
    "        msg = scoop_out_weblinks(msg)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject(mail):\n",
    "    sub = mail['subject'] \n",
    "    if sub == None:\n",
    "        sub = ' '         \n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_msg_and_sub(email_file):\n",
    "    mail = read_email(email_file)\n",
    "    msg = get_msg(mail)\n",
    "    sub = get_subject(mail)\n",
    "    msg_and_sub = msg + ' \\n ' + sub\n",
    "    return msg_and_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_of_email(email_file):\n",
    "    tag = 0\n",
    "    if 'ham' in (email_file.lower().split('/')):\n",
    "        tag = 1\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_and_sub = np.array([])\n",
    "y = np.array([])\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    msg_and_sub  = np.hstack((msg_and_sub, extract_msg_and_sub(file_list[i])))\n",
    "    y = np.hstack((y, get_tag_of_email(file_list[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_msg_and_sub, test_msg_and_sub, y_train, y_test = train_test_split(msg_and_sub, y, train_size = 0.6, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of Data:\n",
    "Once we have the messages and subject extracted from emails, the next step would be transforming these messages into vectors. One way to do that would be:\n",
    "- Splitting the whole message into words,\n",
    "- Appending all the words into an array.\n",
    "- And dropping repeated words to give the vocabulary.\n",
    "- Converting the email into vector can be done by taking a vector of size V (length of vocabulary), and:\n",
    " - for words in vocabulary if they are in the email, we put 1\n",
    " - for words in vocabulary, if they are not in the email, we put 0.\n",
    "\n",
    "The other (and convinient) way to transform the data to vectors is by using:\n",
    "- CountVectorizer \n",
    "\n",
    "The former process takes a lot of time to run. But if we want to use spell-check, it can be easily done if we use the first way of vectorizing. \n",
    "\n",
    "Note: While using *CountVectorizer*, define *min_df* feature to be equal to 1/train_size. It improved the accuracy from . This might be because if we have emails which has a time or date of rendezvous, or the name of the sender, or any such words or letter which once in the whole data set, then these will add little information about whether the email is spam or ham. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=1/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = cv.fit_transform(np.array(train_msg_and_sub))\n",
    "x_test = cv.transform(np.array(test_msg_and_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model:\n",
    " Initially, I have used the first way of vectorizing the data. This way does not care for the repetition of words. It is only concerned about the presence of a word. So, I have used Gaussian Naive Bayes (**GaussianNB**) classifier.<br>\n",
    " *CountVectorizer*, however, does consider repetitions. *GuassianNB* fails to consider the frequency of words into account. So, I switched to **MultinomialNB** classifier, which exactly does that. This improved the classification accuracy from 96.5% for *GaussianNB* to 98.6% for *MultinomialNB*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_multinb = MultinomialNB()\n",
    "model_multinb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model_multinb.predict(x_test)\n",
    "y_pred_train = model_multinb.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(prediction, positive_class):\n",
    "    con_mat = confusion_matrix(prediction, positive_class)\n",
    "    \n",
    "    true_positive = con_mat[1,1]\n",
    "    true_negative = con_mat[0,0]\n",
    "    predicted_positive = (prediction == 1).sum()\n",
    "    positive_labels = (positive_class == 1).sum()\n",
    "    \n",
    "    accuracy = (true_positive + true_negative)/len(positive_class)\n",
    "    recall = true_positive/positive_labels\n",
    "    precision = true_positive/predicted_positive\n",
    "    f1_score = 2*recall*precision/(recall + precision)\n",
    "    \n",
    "    return accuracy, recall, precision, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, recall, precision, f1_score = get_metrics(y_pred, y_test)\n",
    "acc_train, recall_train, prec_train, f1_train = get_metrics(y_pred_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Metrics on test: \n",
      " \n",
      " Accuracy =  0.984587488667 \n",
      " Recall =  0.990040460629 \n",
      " Precision =  0.988809449798 \n",
      " F1_score =  0.989424572317 \n",
      " \n",
      " Metrics on train: \n",
      " \n",
      " Accuracy =  0.991839202055 \n",
      " Recall =  0.99377593361 \n",
      " Precision =  0.995014540922 \n",
      " F1_score =  0.994394851567\n"
     ]
    }
   ],
   "source": [
    "print( ' Metrics on test:','\\n','\\n', 'Accuracy = ', accuracy,'\\n','Recall = ', recall, '\\n', 'Precision = ', precision, '\\n', 'F1_score = ', f1_score, '\\n','\\n','Metrics on train:','\\n','\\n', 'Accuracy = ', acc_train,'\\n','Recall = ', recall_train, '\\n', 'Precision = ', prec_train, '\\n', 'F1_score = ', f1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "- Extract and process the data. i.e. extract the messages, replace the web links with the word 'link'.\n",
    "- Trasform the data into vectors using CountVectorizer.\n",
    "- On these vectors, use MultinomialNB classifier.\n",
    "\n",
    "And that is it. You have a spam classifier of your own. Cheers!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
